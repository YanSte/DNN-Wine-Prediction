{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [DNN]-[Wine] Predicting Wine Quality with a Dense Neural Network (DNN)\n\nIn this notebook, we employ a Dense Neural Network (DNN) to perform a prediction task on the renowned Wine Quality dataset.\n\nThe [Wine Quality dataset](https://archive.ics.uci.edu/ml/datasets/wine+Quality), comprised of extensive wine analyses, assigns each wine a quality score between 0 and 10. Credit for the dataset goes to [Paulo Cortez](http://www3.dsi.uminho.pt/pcortez) from the University of Minho, Guimar√£es, Portugal. You can access this dataset from the [University of California Irvine (UCI) Machine Learning Repository](https://archive-beta.ics.uci.edu/ml/datasets/wine+quality).\n\nThe dataset, due to privacy and logistical issues, only contains physicochemical and sensory variables. Aspects such as grape types, wine brands, and selling prices are excluded. The dataset includes the following features:\n\n- Fixed acidity\n- Volatile acidity\n- Citric acid\n- Residual sugar\n- Chlorides\n- Free sulfur dioxide\n- Total sulfur dioxide\n- Density\n- pH\n- Sulphates\n- Alcohol\n- Quality (score between 0 and 10)\n\nThe notebook is structured into the following sections:\n\n## Objective:\nOur primary aim is to predict wine quality based on the provided analysis data.\n\n## Steps:\n1. **Imports, Constants, & Methods**: Set up the necessary libraries, constants, and methods for the task.\n2. **Data Retrieval**: Fetch the Wine Quality dataset from the provided source.\n3. **Data Preparation**: Conduct necessary preprocessing and data cleaning activities on the dataset.\n4. **Model Creation**: Design and establish a Dense Neural Network (DNN) for predicting wine quality.\n5. **Model Training & Saving**: Train the DNN on the prepared dataset and store the trained model for future reference.\n6. **Model Evaluation**: Gauge the performance of the trained model by evaluating its predictions against the test data.\n7. **Evaluation of Best Saved Model**: Assess the performance of the best model saved during training, by evaluating its predictions on test data.\n8. **Prediction**: Use the best-trained model to generate predictions on new, unseen wine data.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Imports & Constants & Methods","metadata":{}},{"cell_type":"markdown","source":"### 1.1. Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os,sys\n\nfrom IPython.display import Markdown\nfrom importlib import reload\n\n!pip install visualkeras\n\nimport visualkeras","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:55:08.856416Z","iopub.execute_input":"2023-07-15T17:55:08.856781Z","iopub.status.idle":"2023-07-15T17:55:21.256049Z","shell.execute_reply.started":"2023-07-15T17:55:08.856754Z","shell.execute_reply":"2023-07-15T17:55:21.254657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verbosity during training:\n- 0: Silent mode, no output will be displayed during training.\n- 1: Progress bar mode, a progress bar will be displayed to show the progress of each epoch.\n- 2: One line per epoch mode, a concise summary will be displayed for each epoch.\n\n--For the current training configuration:\n-- Batch size: 512, which determines the number of samples processed in each training iteration.\n-- Number of epochs: 16, indicating the total number of times the model will be trained on the entire dataset.\n","metadata":{}},{"cell_type":"markdown","source":"### 1.2. Constants","metadata":{}},{"cell_type":"code","source":"fit_verbosity = 1\ndataset_path  = '/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv'","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:51:58.414213Z","iopub.execute_input":"2023-07-15T17:51:58.415729Z","iopub.status.idle":"2023-07-15T17:51:58.423278Z","shell.execute_reply.started":"2023-07-15T17:51:58.415660Z","shell.execute_reply":"2023-07-15T17:51:58.421793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3. Methods","metadata":{}},{"cell_type":"code","source":"def show_history(\n    history, \n    figsize=(8,6), \n    plot={\"Accuracy\":['accuracy','val_accuracy'], 'Loss':['loss', 'val_loss']}\n):\n    \"\"\"\n    Show history\n    args:\n        history: history\n        figsize: fig size\n        plot: list of data to plot : {<title>:[<metrics>,...], ...}\n    \"\"\"\n    fig_id=0\n    for title,curves in plot.items():\n        plt.figure(figsize=figsize)\n        plt.title(title)\n        plt.ylabel(title)\n        plt.xlabel('Epoch')\n        for c in curves:\n            plt.plot(history.history[c])\n        plt.legend(curves, loc='upper left')\n        plt.show()\n        \n        \ndef show_confusion_matrix(\n    y_true,\n    y_pred,\n    target_names,\n    title='Confusion matrix',\n    cmap=None,\n    normalize=True,\n    figsize=(10, 8),\n    digit_format='{:0.2f}'\n):\n    cm = sklearn.metrics.confusion_matrix( y_true,y_pred, normalize=None, labels=target_names)\n    \n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=figsize)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=90)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, digit_format.format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T18:00:20.075748Z","iopub.execute_input":"2023-07-15T18:00:20.076593Z","iopub.status.idle":"2023-07-15T18:00:20.092090Z","shell.execute_reply.started":"2023-07-15T18:00:20.076538Z","shell.execute_reply":"2023-07-15T18:00:20.090403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.Retrieve data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(dataset_path, header=0,sep=',')\n\ndisplay(data.head(5).style.format(\"{0:.2f}\"))\nprint('Missing Data : ',data.isna().sum().sum(), '  Shape is : ', data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:52:14.440721Z","iopub.execute_input":"2023-07-15T17:52:14.441140Z","iopub.status.idle":"2023-07-15T17:52:14.464813Z","shell.execute_reply.started":"2023-07-15T17:52:14.441097Z","shell.execute_reply":"2023-07-15T17:52:14.462865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Preparing the data\n\n### 3.1. Splitting the data\n\nTo prepare the data for training and validation, we will split it into two parts: 80% for training and 20% for validation. \n\nThe feature data, representing the analysis variables, will be denoted as 'x', while the target variable, indicating the wine quality, will be denoted as 'y'.","metadata":{}},{"cell_type":"code","source":"# train, test\n#\ndata = data.sample(frac=1., axis=0)           # Shuffle\ndata_train = data.sample(frac=0.8, axis=0)    # get 80%\ndata_test = data.drop(data_train.index)       # test = all - train\n\n# x, y (quality is the target variable)\n#\nx_train = data_train.drop('quality', axis=1)\ny_train = data_train['quality']\nx_test = data_test.drop('quality', axis=1)\ny_test = data_test['quality']\nshape = x_train.shape[1]\n\nprint('Original data shape was:', data.shape)\nprint('x_train:', x_train.shape, 'y_train:', y_train.shape)\nprint('x_test:', x_test.shape, 'y_test:', y_test.shape)\nprint('Shape:', shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:53:57.359315Z","iopub.execute_input":"2023-07-15T17:53:57.360037Z","iopub.status.idle":"2023-07-15T17:53:57.377020Z","shell.execute_reply.started":"2023-07-15T17:53:57.359989Z","shell.execute_reply":"2023-07-15T17:53:57.375489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Data normalization\n\n**Note:**\n- It is important to normalize all input data, including both the training and testing datasets.\n- For normalization, we will subtract the mean and divide by the standard deviation.\n- However, it is crucial to avoid using the test data for any calculations, including normalization.\n- Therefore, the mean and standard deviation will only be calculated based on the training data.","metadata":{}},{"cell_type":"code","source":"display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"Before normalization :\"))\n\nmean = x_train.mean()\nstd  = x_train.std()\n\nx_train = (x_train - mean) / std\nx_test  = (x_test  - mean) / std\n\ndisplay(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n\n# Convert ou DataFrame to numpy array\nx_train, y_train = np.array(x_train), np.array(y_train)\nx_test,  y_test  = np.array(x_test),  np.array(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:54:02.711623Z","iopub.execute_input":"2023-07-15T17:54:02.711974Z","iopub.status.idle":"2023-07-15T17:54:02.782568Z","shell.execute_reply.started":"2023-07-15T17:54:02.711952Z","shell.execute_reply":"2023-07-15T17:54:02.781700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Build a model\nMore informations about : \n - [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n - [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n - [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n - [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)","metadata":{}},{"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(keras.layers.Input(shape, name=\"InputLayer\"))\nmodel.add(keras.layers.Dense(64, activation='relu', name='Dense_n1'))\nmodel.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\nmodel.add(keras.layers.Dense(1, name='Output'))\n\nmodel.compile(\n    optimizer = 'rmsprop',\n    loss = 'mse',\n    metrics = ['mae', 'mse']\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:54:06.028969Z","iopub.execute_input":"2023-07-15T17:54:06.029336Z","iopub.status.idle":"2023-07-15T17:54:06.247836Z","shell.execute_reply.started":"2023-07-15T17:54:06.029305Z","shell.execute_reply":"2023-07-15T17:54:06.246598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Train the model\n### 5.1. Summary","metadata":{}},{"cell_type":"code","source":"visualkeras.layered_view(model, legend=True, scale_z=1, scale_xy =20, spacing=80)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:55:24.750426Z","iopub.execute_input":"2023-07-15T17:55:24.750871Z","iopub.status.idle":"2023-07-15T17:55:24.775062Z","shell.execute_reply.started":"2023-07-15T17:55:24.750836Z","shell.execute_reply":"2023-07-15T17:55:24.773608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:55:32.230928Z","iopub.execute_input":"2023-07-15T17:55:32.231345Z","iopub.status.idle":"2023-07-15T17:55:32.252105Z","shell.execute_reply.started":"2023-07-15T17:55:32.231310Z","shell.execute_reply":"2023-07-15T17:55:32.250891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2. Add callback","metadata":{}},{"cell_type":"code","source":"os.makedirs('./models', mode=0o750, exist_ok=True)\nsave_dir = \"./models/best_model.h5\"\n\nsavemodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:55:51.614154Z","iopub.execute_input":"2023-07-15T17:55:51.614536Z","iopub.status.idle":"2023-07-15T17:55:51.621346Z","shell.execute_reply.started":"2023-07-15T17:55:51.614510Z","shell.execute_reply":"2023-07-15T17:55:51.620168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3. Train it","metadata":{}},{"cell_type":"code","source":"history = model.fit(x_train,\n                    y_train,\n                    epochs          = 100,\n                    batch_size      = 10,\n                    verbose         = fit_verbosity,\n                    validation_data = (x_test, y_test),\n                    callbacks       = [savemodel_callback])","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:55:53.345214Z","iopub.execute_input":"2023-07-15T17:55:53.345645Z","iopub.status.idle":"2023-07-15T17:56:25.280617Z","shell.execute_reply.started":"2023-07-15T17:55:53.345613Z","shell.execute_reply":"2023-07-15T17:56:25.279244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Evaluate\n### 6.1. Model evaluation\n\n**Mean Absolute Error (MAE)** is a metric used to evaluate the accuracy of a regression model. It measures the average absolute difference between the predicted values and the actual values. \n\nA MAE value of 3 indicates that, on average, the predictions deviate from the actual values by `$3k` (3.000 units of currency). In other words, the model's average prediction error is $3k.","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(x_test, y_test, verbose=0)\n\nprint('x_test / loss      : {:5.4f}'.format(score[0]))\nprint('x_test / mae       : {:5.4f}'.format(score[1]))\nprint('x_test / mse       : {:5.4f}'.format(score[2]))","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:57:40.716923Z","iopub.execute_input":"2023-07-15T17:57:40.717286Z","iopub.status.idle":"2023-07-15T17:57:40.802114Z","shell.execute_reply.started":"2023-07-15T17:57:40.717264Z","shell.execute_reply":"2023-07-15T17:57:40.800821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2. Training history","metadata":{}},{"cell_type":"code","source":"print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:58:01.245618Z","iopub.execute_input":"2023-07-15T17:58:01.246012Z","iopub.status.idle":"2023-07-15T17:58:01.251690Z","shell.execute_reply.started":"2023-07-15T17:58:01.245978Z","shell.execute_reply":"2023-07-15T17:58:01.250416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_history( \n    history, \n    plot={\n        'MSE' :['mse', 'val_mse'],\n        'MAE' :['mae', 'val_mae'],\n        'LOSS':['loss','val_loss']\n    })","metadata":{"execution":{"iopub.status.busy":"2023-07-15T18:01:10.996027Z","iopub.execute_input":"2023-07-15T18:01:10.996494Z","iopub.status.idle":"2023-07-15T18:01:11.734065Z","shell.execute_reply.started":"2023-07-15T18:01:10.996461Z","shell.execute_reply":"2023-07-15T18:01:11.732176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During our training, the best result achieved was a low training loss and a low validation loss. However, we observed that there was overfitting in the model.\n\nOverfitting occurs when the model performs well on the training data but fails to generalize well to unseen data. It often results in a large gap between the training loss and the validation loss. In our case, we noticed that the model's performance on the training data was significantly better than its performance on the validation data, indicating overfitting.\n\nDuring the training process, we implemented a callback mechanism to save the best-performing model. The callback monitored the validation loss during each epoch and automatically saved the model when it achieved the lowest validation loss.\n\nBy saving the best model, we ensured that we captured the model's parameters at the point where it exhibited the highest level of accuracy on the validation data. This allows us to use this saved model for future predictions or further analysis, confident that it represents the model with the best performance during the training process.","metadata":{}},{"cell_type":"markdown","source":"## 7. Evaluate the best model","metadata":{}},{"cell_type":"markdown","source":"### 7.1. Reload model","metadata":{}},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model('./models/best_model.h5')\nloaded_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T18:11:35.347732Z","iopub.execute_input":"2023-07-15T18:11:35.348142Z","iopub.status.idle":"2023-07-15T18:11:35.440389Z","shell.execute_reply.started":"2023-07-15T18:11:35.348116Z","shell.execute_reply":"2023-07-15T18:11:35.439007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2. Evaluate","metadata":{}},{"cell_type":"code","source":"score = loaded_model.evaluate(x_test, y_test, verbose=0)\n\nprint('x_test / loss : {:5.4f}'.format(score[0]))\nprint('x_test / mae : {:5.4f}'.format(score[1]))\nprint('x_test / mse : {:5.4f}'.format(score[2]))","metadata":{"execution":{"iopub.status.busy":"2023-07-15T18:11:47.121597Z","iopub.execute_input":"2023-07-15T18:11:47.121958Z","iopub.status.idle":"2023-07-15T18:11:47.194075Z","shell.execute_reply.started":"2023-07-15T18:11:47.121929Z","shell.execute_reply":"2023-07-15T18:11:47.192475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Make a prediction","metadata":{}},{"cell_type":"code","source":"# Pick n entries from our test set\nn = 200\nii = np.random.randint(1,len(x_test),n)\nx_sample = x_test[ii]\ny_sample = y_test[ii]","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:58:30.080988Z","iopub.execute_input":"2023-07-15T17:58:30.081359Z","iopub.status.idle":"2023-07-15T17:58:30.088354Z","shell.execute_reply.started":"2023-07-15T17:58:30.081328Z","shell.execute_reply":"2023-07-15T17:58:30.086556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a predictions\ny_pred = loaded_model.predict( x_sample, verbose=2 )","metadata":{"execution":{"iopub.status.busy":"2023-07-15T17:58:31.565708Z","iopub.execute_input":"2023-07-15T17:58:31.566161Z","iopub.status.idle":"2023-07-15T17:58:31.739123Z","shell.execute_reply.started":"2023-07-15T17:58:31.566115Z","shell.execute_reply":"2023-07-15T17:58:31.738094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show it\nprint('Wine    Prediction   Real   Delta')\nfor i in range(n):\n    pred   = y_pred[i][0]\n    real   = y_sample[i]\n    delta  = real-pred\n    print(f'{i:03d}        {pred:.2f}       {real}      {delta:+.2f} ')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T18:08:34.213877Z","iopub.execute_input":"2023-07-15T18:08:34.214222Z","iopub.status.idle":"2023-07-15T18:08:34.221500Z","shell.execute_reply.started":"2023-07-15T18:08:34.214197Z","shell.execute_reply":"2023-07-15T18:08:34.220502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n\nThe creation of this document was greatly influenced by the following key sources of information:\n\n1. Cerdeira, A., Almeida, F., Matos, T., & Reis, J., Viticulture Commission of the Vinho Verde Region (CVRVV), Porto, Portugal, 2009. The [Wine Quality datasets](https://archive.ics.uci.edu/ml/datasets/wine+Quality) available at the [University of California Irvine (UCI) Machine Learning Repository](https://archive-beta.ics.uci.edu/ml/datasets/wine+quality) were an invaluable resource.\n2. [Paulo Cortez](http://www3.dsi.uminho.pt/pcortez), University of Minho, Guimar√£es, Portugal. His contributions to the field and online resources were a significant source of information and learning material.\n3. [Fidle](https://gricad-gitlab.univ-grenoble-alpes.fr/talks/fidle/-/wikis/home) - An informative guide that provides in-depth explanations and examples on various data science topics.","metadata":{}}]}